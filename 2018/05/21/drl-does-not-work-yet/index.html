<!DOCTYPE html>
<html lang="zh-Hans">


<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, user-scalable=no">
  <title>
    drl does not work yet | 上药三品，神与气精
  </title>
  <meta name="description" content="曾因酒醉鞭名马 生怕情多累美人">
  
  <meta name="keywords" content="
  deep learning
  ">
  
  <meta name="author" content="John Cheung">

  <meta http-equiv="Cache-Control" content="no-transform"/>
  <meta http-equiv="Cache-Control" content="no-siteapp">

  <link rel="icon" type="image/x-icon" href="undefined">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet"
        href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  

  

  <script src="//cdnjs.cloudflare.com/ajax/libs/vue/1.0.25-csp/vue.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.11.2/moment.min.js"></script>
</head>

<body id="replica-app">

<nav class="navbar-wrapper">
  <div class="navbar">
    <div class="container clearfix">
      <a href="/" class="navbar-logo"><i class="fa fa-github"></i></a>

      <div class="navbar-search float-left desktop-only">
        <div class="navbar-search-form">
          <label for="gsc-i-id1">This website</label>
          <div id="google-search">
            <gcse:search></gcse:search>
          </div>
        </div>
      </div>

      <ul class="navbar-nav float-left">
        
        <li><a href="/archives">Archives</a></li>
        
        
        <li><a href="/categories">Categories</a></li>
        
        
        <li><a href="/tags">Tags</a></li>
        
        
        <li class="desktop-only"><a href="/atom.xml" target="_blank">RSS</a></li>
        
      </ul>

      <ul class="navbar-nav user-nav float-right desktop-only">
        <li class="user-nav-notification">
          <a><span class="user-nav-unread"></span><i class="fa fa-bell"></i></a>
        </li>
        <li>
          <a><i class="fa fa-plus"></i> <i class="fa fa-caret-down"></i></a>
        </li>
        <li class="user-nav-logo">
          <a><img src="/images/pingan.jpeg"> <i class="fa fa-caret-down"></i></i></a>
        </li>
      </ul>
    </div>
  </div>
</nav>

<div class="main-container">
  <header class="header-wrapper desktop-only">
  <div class="container header-site-detail">
    <ul class="header-toolbar">
      <li class="clearfix">
        <a href="/archives" class="header-toolbar-left"><i
                  class="fa fa-file-text"></i> Posts </a>
        <a href="/archives"
           class="header-toolbar-right"> 315 </a>
      </li>
      <li>
        <a href="/tags" class="header-toolbar-left"><i
                  class="fa fa-tags"></i> Tags </a>
        <a href="/tags"
           class="header-toolbar-right"> 28 </a>
      </li>
      <li>
        <a href="/categories" class="header-toolbar-left"><i
                  class="fa fa-folder-open"></i> Categories </a>
        <a href="/categories"
           class="header-toolbar-right"> 24 </a>
      </li>
    </ul>
    <h2 class="header-title">
      <i class="fa fa-book text-muted"></i>
      <a href="/">上药三品，神与气精</a>
      
      
    </h2>
  </div>

  <div class="container">
    <div class="header-tab-wrapper clearfix">
      <span class="header-tab header-tab-selected"><i class="fa fa-thumbs-o-up"></i> Like</span>
      <span class="header-tab"><i class="fa fa-share-alt"></i> Share</span>
      <span class="header-tab"><i class="fa fa-comments-o"></i> Discussion</span>
      <span class="header-tab"><i class="fa fa-bookmark-o"></i> Bookmark </span>
      <span class="header-tab"><i class="fa fa-smile-o"></i> Smile <i class="fa fa-caret-down"></i></span>
    </div>
  </div>
</header>


<div class="post-container container">
  <h3>
    <i class="fa fa-user-o"></i>
    John Cheung

    <span class="post-date float-right" title="{{moment(1526900912000).format('MMM DD, YYYY, h:mm:ss A')}}">
      <i class="fa fa-pencil-square-o"></i>
      {{moment(1526900912000).fromNow()}}
    </span>
  </h3>

  <article class="post-content">
    <h1>drl does not work yet</h1>
    <h2 id="劝退文"><a href="#劝退文" class="headerlink" title="劝退文"></a>劝退文</h2><p>先看一下作者的背景。作者叫 Alex Irpan，现为谷歌大脑机器人团队的软件工程师。他从伯克利拿到的计算机科学本科学位，本科的时候曾经在伯克利人工智能实验室（Berkeley AI Research (BAIR) Lab）进行本科科研，导师是 DRL 大牛 Pieter Abbeel，他还和 John Schulman 工作过。</p>
<p>这篇文章一上来就指出深度强化学习是个大坑。它的成功案例其实很少，但每个都太有名了，例如用 Deep Q Network（DQN）在 Atari games 上用原始像素图片作为状态达到甚至超越人类专家的表现、通过左右互搏（self-play）等方式在围棋上碾压人类、大大降低了谷歌能源中心的能耗等等。造成的结果就是没有从事过深度强化学习的研究人员对它产生了很大的错觉，高估了它的能力，低估了它的难度。</p>
<p>强化学习本身是一个非常通用的人工智能范式，在直觉上让人觉得非常适合用来模拟各种时序决策任务，如语音、文本类任务。当它和深度神经网络这种只要给我足够层和足够多的神经元，可以逼近任何函数的非线性函数近似模型结合在一起感觉要上天啊，无怪乎 DeepMind 经常号称人工智能=深度学习+强化学习。</p>
<p>然而 Alex 告诉我们别急，让我们先来审视一些问题：</p>
<p>1.它的样本利用率非常低。换言之为了让模型的表现达到一定高度需要极为大量的训练样本。</p>
<p>2.最终表现很多时候不够好。在很多任务上用非强化学习甚至非学习的其它方法，如基于模型的控制（model based control），线性二次型调节器（Linear Quadratic Regulator）等等可以获得好得多的表现。最气人的是这些模型很多时候样本利用率还高。当然这些模型有的时候会有一些假设比如有训练好的模型可以模仿，比如可以进行蒙特卡洛树搜索等等。</p>
<p>3.DRL 成功的关键离不开一个好的奖励函数（reward function），然而这种奖励函数往往很难设计。在 Deep Reinforcement Learning That Matters 作者提到有时候把奖励乘以一个常数模型表现就会有天和地的区别。但奖励函数的坑爹之处还不止如此。奖励函数的设计需要保证：</p>
<p>加入了合适的先验，良好的定义了问题和在一切可能状态下的对应动作。坑爹的是模型很多时候会找到作弊的手段。Alex 举的一个例子是有一个任务需要把红色的乐高积木放到蓝色的乐高积木上面，奖励函数的值基于红色乐高积木底部的高度而定。结果一个模型直接把红色乐高积木翻了一个底朝天。仔啊，你咋学坏了，阿爸对你很失望啊。</p>
<p>奖励函数的值太过稀疏。换言之大部分情况下奖励函数在一个状态返回的值都是 0。这就和我们人学习也需要鼓励，学太久都没什么回报就容易气馁。都说 21 世纪是生物的世纪，怎么我还没感觉到呢？21 世纪才刚开始呢。我等不到了啊啊啊啊啊。</p>
<p>有的时候在奖励函数上下太多功夫会引入新的偏见（bias）。</p>
<p>要找到一个大家都使用而又具有好的性质的奖励函数。这里Alex没很深入地讨论，但链接了一篇陶神（Terence Tao）的博客，大家有兴趣可以去看下。</p>
<p>4.局部最优/探索和剥削（exploration vs. exploitation）的不当应用。Alex举的一个例子是有一个连续控制的环境里，一个类似马的四足机器人在跑步，结果模型不小心多看到了马四脚朝天一顿乱踹后结果较好的情况，于是你只能看到四脚朝天的马了。</p>
<p>5.对环境的过拟合。DRL 少有在多个环境上玩得转的。你训练好的 DQN 在一个 Atari game上work 了，换一个可能就完全不 work。即便你想要做迁移学习，也没有任何保障你能成功。</p>
<p>6.不稳定性。</p>
<p>读 DRL 论文的时候会发现有时候作者们会给出一个模型表现随着尝试 random seed 数量下降的图，几乎所有图里模型表现最终都会降到 0。相比之下在监督学习里不同的超参数或多或少都会表现出训练带来的变化，而 DRL 里运气不好可能很长时间你模型表现的曲线都没有任何变化，因为完全不 work。</p>
<p>即便知道了超参数和随机种子，你的实现只要稍有差别，模型的表现就可以千差万别。这可能就是 Deep Reinforcement Learning That Matters 一文里 John Schulman 两篇不同文章里同一个算法在同一个任务上表现截然不同的原因。</p>
<p>即便一切都很顺利，从我个人的经验和之前同某 DRL 研究人员的交流来看只要时间一长你的模型表现就可能突然从很好变成完全不 work。原因我不是完全确定，可能和过拟合和 variance 过大有关。</p>
<p>特别是上述第六点，几乎是灾难性的。作者提到自己实习的时候一开始实现  Normalized Advantage Function (NAF)，为了找出 Theano 本身的 bugs 花了六周，这还是在 NAF 作者就在他旁边可以供他骚扰的情况下的结果。原因就是DRL的算法很多时候在没找好超参数的情况下就是不 work 的，所以你很难判断自己的代码到底有没有 bug 还是运气不好。</p>
<p>作者也回顾了 DRL 成功的案例，他认为 DRL 成功的案例其实非常少，大体包括：</p>
<p>各类游戏：Atari Games, Alpha Go/Alpha Zero/Dota2 1v1/超级马里奥/日本将棋，其实还应该有 DRL 最早的成功案例，93年的西洋双陆棋（backgammon）。</p>
<p>DeepMind 的跑酷机器人。</p>
<p>为 Google 的能源中心节能。</p>
<p>Google 的 AutoML。</p>
<p>作者认为从这些案例里获得的经验教训是 DRL 可能在有以下条件的情况下更可能有好的表现，条件越多越好：</p>
<ul>
<li><p>数据获取非常容易，非常 cheap。</p>
</li>
<li><p>不要急着一上来就攻坚克难，可以从简化的问题入手。</p>
</li>
<li><p>可以进行左右互搏。</p>
</li>
<li><p>奖励函数容易定义。</p>
</li>
<li><p>奖励信号非常多，反馈及时。</p>
</li>
</ul>
<p>他也指出了一些未来潜在的发展方向和可能性：</p>
<ul>
<li><p>局部最优或许已经足够好。未来某些研究可能会指出我们不必过于担心大部分情况下的局部最优。因为他们比起全局最优并没有差很多。</p>
</li>
<li><p>硬件为王。在硬件足够强的情况下我们或许就不用那么在乎样本利用率了，凡事硬刚就可以有足够好的表现。各种遗传算法玩起来。</p>
</li>
<li><p>人为添加一些监督信号。在环境奖励出现频次太低的情况下可以引入自我激励（intrinsic reward）或者添加一些辅助任务，比如DeepMind就很喜欢这套，之前还写了一篇 Reinforcement Learning with Unsupervised Auxiliary Tasks（<a href="https://arxiv.org/abs/1611.05397）" target="_blank" rel="external">https://arxiv.org/abs/1611.05397）</a> 。LeCun 不是嫌蛋糕上的樱桃太少吗，让我们多给他点樱桃吧！</p>
</li>
<li><p>更多融合基于模型的学习从而提高样本使用率。这方面的尝试其实已经有很多了，具体可以去看 Alex 提到的那些工作。但还远不够成熟。</p>
</li>
<li><p>仅仅把 DRL 用于 fine-tuning。比如最初 Alpha Go 就是以监督学习为主，以强化学习为辅。</p>
</li>
<li><p>自动学习奖励函数。这涉及到 inverse reinforcement learning 和 imitation learning。</p>
</li>
<li><p>迁移学习和强化学习的进一步结合。</p>
</li>
<li><p>好的先验。</p>
</li>
<li><p>有的时候复杂的任务反而更容易学习。Alex 提到的例子是 DeepMind 经常喜欢让模型学习很多同一环境的变种来减小对环境的过拟合。我觉得这也涉及 curriculum learning，即从简单的任务开始逐步加深难度。可以说是层层递进的迁移学习。另外一个可能的解释是很多时候人觉得困难的任务和机器觉得困难的任务是相反的。比如人觉得倒水很简单，你让机器人用学习的路子去学倒水就可以很难。但反过来人觉得下围棋很简单而机器学习模型却在下围棋上把人击败了。</p>
</li>
</ul>
<p>最后 Alex 总体还是非常乐观的。他说尽管现在有很多困难，使得 DRL 或许还不是一个强壮（robust）到所有人都可以轻易加入的研究领域并且很多时候一些问题用DRL远没有监督学习简单和表现好，但或许过几年你再回来 DRL 就 work 了也未知啊。这还是很振奋人心的。田渊栋老师也表达过类似的想法，觉得正因为这个领域还不够成熟所以还有很多机会。他们都是了不起的研究人员。</p>

  </article>
</div>


    




</div>

<div class="footer-wrapper container">
  <footer class="footer clearfix">
    <div class="clearfix">
    <a href="https://zhangdavids.github.io" class="footer-logo">
      <i class="fa fa-github"></i>
    </a>
    <ul class="footer-social-link">
      <li>© 2018 John Cheung</li>
      <li><a href="https://zhangdavids.github.io">Home</a></li>
      
    </ul>
    <div class="footer-theme-info">
      Theme <a href="//github.com/sabrinaluo/hexo-theme-replica">Replica</a>
      by <a href="//github.com/sabrinaluo">Hiitea</a> ❤ Powered by Hexo
    </div>
    </div>
    
  </footer>
</div>




<script src="/js/main.js"></script>

</body>
</html>
