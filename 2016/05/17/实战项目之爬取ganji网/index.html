<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>实战项目之爬取ganji网 | 上药三品，神与气精</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="爬取分类信息网站龙头  ganji网。  
首先呢，来看看结构 
我们需要来看看首页那里的所有类目，里面包含20项基本上所有的帖子都会被分类在这20个下方，根据时间被分配不同的id，像这样http://bj.ganji.com/ershoubijibendiannao/2111373477x.htm当然后面加了个x。。。  

第一步先把这20个类目的地址爬取出来。

12345678910111">
<meta property="og:type" content="article">
<meta property="og:title" content="实战项目之爬取ganji网">
<meta property="og:url" content="http://yoursite.com/2016/05/17/实战项目之爬取ganji网/index.html">
<meta property="og:site_name" content="上药三品，神与气精">
<meta property="og:description" content="爬取分类信息网站龙头  ganji网。  
首先呢，来看看结构 
我们需要来看看首页那里的所有类目，里面包含20项基本上所有的帖子都会被分类在这20个下方，根据时间被分配不同的id，像这样http://bj.ganji.com/ershoubijibendiannao/2111373477x.htm当然后面加了个x。。。  

第一步先把这20个类目的地址爬取出来。

12345678910111">
<meta property="og:image" content="http://o7b8j5zne.bkt.clouddn.com/ganji%E5%9B%BE1.png">
<meta property="og:updated_time" content="2016-05-17T08:04:22.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="实战项目之爬取ganji网">
<meta name="twitter:description" content="爬取分类信息网站龙头  ganji网。  
首先呢，来看看结构 
我们需要来看看首页那里的所有类目，里面包含20项基本上所有的帖子都会被分类在这20个下方，根据时间被分配不同的id，像这样http://bj.ganji.com/ershoubijibendiannao/2111373477x.htm当然后面加了个x。。。  

第一步先把这20个类目的地址爬取出来。

12345678910111">
<meta name="twitter:image" content="http://o7b8j5zne.bkt.clouddn.com/ganji%E5%9B%BE1.png">
  
    <link rel="alternate" href="/atom.xml" title="上药三品，神与气精" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">上药三品，神与气精</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">寿本乎仁 乐生于智　勤能补拙 俭可养廉</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-实战项目之爬取ganji网" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/05/17/实战项目之爬取ganji网/" class="article-date">
  <time datetime="2016-05-17T03:06:04.000Z" itemprop="datePublished">2016-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      实战项目之爬取ganji网
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>爬取分类信息网站龙头  ganji网。  </p>
<p>首先呢，来看看结构<br><img src="http://o7b8j5zne.bkt.clouddn.com/ganji%E5%9B%BE1.png" alt="图1"> </p>
<p>我们需要来看看首页那里的所有类目，里面包含20项<br>基本上所有的帖子都会被分类在这20个下方，根据时间被分配不同的id，像这样<br><a href="http://bj.ganji.com/ershoubijibendiannao/2111373477x.htm" target="_blank" rel="external">http://bj.ganji.com/ershoubijibendiannao/2111373477x.htm</a><br>当然后面加了个x。。。  </p>
<ul>
<li>第一步<br>先把这20个类目的地址爬取出来。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">start_url = <span class="string">'http://bj.ganji.com/wu/'</span></span><br><span class="line">url_host = <span class="string">'http://bj.ganji.com'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_index_url</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="comment"># url = start_url</span></span><br><span class="line">    wb_data = requests.get(url)</span><br><span class="line">    soup = BeautifulSoup(wb_data.text, <span class="string">'lxml'</span>)</span><br><span class="line">    links = soup.select(<span class="string">'.fenlei &gt; dt &gt; a'</span>)</span><br><span class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">        page_url = url_host + link.get(<span class="string">'href'</span>)</span><br><span class="line">        print(page_url)</span><br><span class="line"></span><br><span class="line">get_index_url(start_url)</span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line">程序的运行结果是<span class="number">20</span>个链接，</span><br></pre></td></tr></table></figure>
<p><a href="http://bj.ganji.com/jiaju/" target="_blank" rel="external">http://bj.ganji.com/jiaju/</a><br><a href="http://bj.ganji.com/rirongbaihuo/" target="_blank" rel="external">http://bj.ganji.com/rirongbaihuo/</a><br><a href="http://bj.ganji.com/shouji/" target="_blank" rel="external">http://bj.ganji.com/shouji/</a><br><a href="http://bj.ganji.com/shoujihaoma/" target="_blank" rel="external">http://bj.ganji.com/shoujihaoma/</a><br><a href="http://bj.ganji.com/bangong/" target="_blank" rel="external">http://bj.ganji.com/bangong/</a><br><a href="http://bj.ganji.com/nongyongpin/" target="_blank" rel="external">http://bj.ganji.com/nongyongpin/</a><br><a href="http://bj.ganji.com/jiadian/" target="_blank" rel="external">http://bj.ganji.com/jiadian/</a><br><a href="http://bj.ganji.com/ershoubijibendiannao/" target="_blank" rel="external">http://bj.ganji.com/ershoubijibendiannao/</a><br><a href="http://bj.ganji.com/ruanjiantushu/" target="_blank" rel="external">http://bj.ganji.com/ruanjiantushu/</a><br><a href="http://bj.ganji.com/yingyouyunfu/" target="_blank" rel="external">http://bj.ganji.com/yingyouyunfu/</a><br><a href="http://bj.ganji.com/diannao/" target="_blank" rel="external">http://bj.ganji.com/diannao/</a><br><a href="http://bj.ganji.com/xianzhilipin/" target="_blank" rel="external">http://bj.ganji.com/xianzhilipin/</a><br><a href="http://bj.ganji.com/fushixiaobaxuemao/" target="_blank" rel="external">http://bj.ganji.com/fushixiaobaxuemao/</a><br><a href="http://bj.ganji.com/meironghuazhuang/" target="_blank" rel="external">http://bj.ganji.com/meironghuazhuang/</a><br><a href="http://bj.ganji.com/shuma/" target="_blank" rel="external">http://bj.ganji.com/shuma/</a><br><a href="http://bj.ganji.com/laonianyongpin/" target="_blank" rel="external">http://bj.ganji.com/laonianyongpin/</a><br><a href="http://bj.ganji.com/xuniwupin/" target="_blank" rel="external">http://bj.ganji.com/xuniwupin/</a><br><a href="http://bj.ganji.com/qitawupin/" target="_blank" rel="external">http://bj.ganji.com/qitawupin/</a><br><a href="http://bj.ganji.com/ershoufree/" target="_blank" rel="external">http://bj.ganji.com/ershoufree/</a><br><a href="http://bj.ganji.com/wupinjiaohuan/" target="_blank" rel="external">http://bj.ganji.com/wupinjiaohuan/</a></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">把这20个链接放入程序中，成为channel\_list。  </span><br><span class="line">这也就是我们的第一个小程序 channel\_extracing.py。  </span><br><span class="line"></span><br><span class="line">---  </span><br><span class="line"><span class="bullet">- </span>第二步  </span><br><span class="line">从大的分类下获取单个帖子的链接，先来分析下页面结构，   </span><br><span class="line">![<span class="string">图2</span>](<span class="link">http://o7b8j5zne.bkt.clouddn.com/ganji%E5%9B%BE2.png</span>) </span><br><span class="line"></span><br><span class="line">以二手笔记本电脑为例，可以看到后面结构为o＋page，这里o代表的是个人发布的，后面是page。  </span><br><span class="line">那么首先考虑的是构造一个爬虫来抓取links</span><br></pre></td></tr></table></figure>
<h1 id="spider-1"><a href="#spider-1" class="headerlink" title="spider 1"></a>spider 1</h1><p>def get_links_from(channel, pages, who_sells=’o’):</p>
<pre><code># http://bj.ganji.com/ershoubijibendiannao/o3/
# o for personal a for merchant
list_view = &apos;{}{}{}/&apos;.format(channel, str(who_sells), str(pages))
wb_data = requests.get(list_view,headers=headers,proxies=proxies)
soup = BeautifulSoup(wb_data.text, &apos;lxml&apos;)
if soup.find(&apos;ul&apos;, &apos;pageLink&apos;):
    for link in soup.select(&apos;dd.feature div ul li a&apos;):
        item_link = link.get(&apos;href&apos;)
        if item_link != &quot;javascript:&quot;:                
            url_list.insert_one({&apos;url&apos;: item_link})
            print(item_link)
        # return urls
else:
    # It&apos;s the last page !
    pass
</code></pre><figure class="highlight qml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里注意一下加了一个判断，判断这一页不是最后一页，就是看下方有无pageLink，俗称也就是翻页器类似的，还有就是直接加上了headers部分和proxy部分反爬取。  </span><br><span class="line"></span><br><span class="line">函数的结果是urls，并考虑使用mongo数据库来存储这个<span class="built_in">url</span>\_list。  </span><br><span class="line"></span><br><span class="line">接下来就是访问单个的<span class="built_in">url</span>来抓取里面的数据信息了。</span><br></pre></td></tr></table></figure>
<h1 id="spider-2"><a href="#spider-2" class="headerlink" title="spider 2"></a>spider 2</h1><p>def get_item_info_from(url,data=None):<br>    wb_data = requests.get(url,headers=headers)<br>    if wb_data.status_code == 404:<br>        pass<br>    else:<br>        soup = BeautifulSoup(wb_data.text, ‘lxml’)<br>        data = {<br>            ‘title’:soup.title.text.strip(),<br>            ‘price’:soup.select(‘.f22.fc-orange.f-type’)[0].text.strip(),<br>            ‘pub_date’:soup.select(‘.pr-5’)[0].text.strip().split(‘ ‘)[0],<br>            ‘area’:list(map(lambda x:x.text,soup.select(‘ul.det-infor &gt; li:nth-of-type(3) &gt; a’))),<br>            ‘cates’:list(soup.select(‘ul.det-infor &gt; li:nth-of-type(1) &gt; span’)[0].stripped_strings),<br>            ‘url’:url<br>        }<br>        print(data)<br>        item_info.insert_one(data)<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">看图，单个页面我们考虑抓取，标题title 价格price 发布时间pub\_data 交易地点area 类型cates 另外再考虑把url也加入里面。大概先这样咯。  </span><br><span class="line">![<span class="string">图3</span>](<span class="link">http://o7b8j5zne.bkt.clouddn.com/ganji%E5%9B%BE3.png</span>)  </span><br><span class="line"></span><br><span class="line">把程序组织一下 组成第二个py程序 page\_parsing.py  </span><br><span class="line">当然详情页抓取的数据我们也放到mongo的数据库中。  </span><br><span class="line"></span><br><span class="line">到这里初步小结一下，实现了哪些呢？先是从ganji的首页获取分类，20个分类  </span><br><span class="line">然后在这20个分类下，获取单个帖子的详情页，然后通过详情页去抓取想要的数据。  </span><br><span class="line">爬取的初步流程完成了，接下来就是包装，    </span><br><span class="line">写个主函数，定义下我们抓取多少页的大分类，先url\<span class="emphasis">_list然后item\_</span>info都存入到mongo的数据库中。  </span><br><span class="line">ps 考虑写一个计数的函数观察我们抓取的信息数目。  </span><br><span class="line">  </span><br><span class="line">---  </span><br><span class="line"><span class="bullet">- </span>第三步   </span><br><span class="line">主函数的分析</span><br></pre></td></tr></table></figure></p>
<p>from multiprocessing import Pool<br>from page_parsing import get_item_info_from,url_list,item_info,get_links_from<br>from channel_extracing import channel_list<br><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">也就是前面的两个函数，我们实现了几个功能：</span><br><span class="line">  </span><br><span class="line">channel<span class="symbol">\_</span>list   </span><br><span class="line">两个方法 get<span class="symbol">\_</span>item<span class="symbol">\_</span>info<span class="symbol">\_</span>from	 get<span class="symbol">\_</span>links<span class="symbol">\_</span>from  </span><br><span class="line">两个mongo数据库的collections url<span class="symbol">\_</span>list item<span class="symbol">\_</span>info  </span><br><span class="line"></span><br><span class="line">另外这里使用多进程来加快抓取的速度</span><br></pre></td></tr></table></figure></p>
<p>def get_all_links_from(channel):<br>    for i in range(1,100):<br>        get_links_from(channel,i)<br><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意前面定义的函数我们定义了页面，这里抓取<span class="number">1</span>-<span class="number">99</span>页，当然也许部分分类商品多，<span class="number">99</span>不足，部分分类商品不是很多，我们也已经有判断会<span class="keyword">pass</span>掉。。</span><br></pre></td></tr></table></figure></p>
<p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    pool = Pool(processes=6)</p>
<pre><code># pool = Pool()
pool.map(get_all_links_from,channel_list.split())
pool.close()
pool.join()
</code></pre><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">开6个进程进行爬爬爬，先存url，再根据url访问抓取。    </span><br><span class="line"></span><br><span class="line">-<span class="ruby">--  </span><br><span class="line"></span>-<span class="ruby"> 第四步    </span><br><span class="line"></span>写上一个计数的函数，这样可以在运行main函数存入数据库时并打印链接的同时，统计已经写了多少数据进去。</span><br></pre></td></tr></table></figure>
<p>import time<br>from page_parsing import url_list</p>
<p>while True:<br>    print(url_list.find().count())<br>    time.sleep(5)<br>```  </p>
<hr>
<ul>
<li>最后<br>附上项目github地址    <a href="https://github.com/zhangdavids/myganji" target="_blank" rel="external">项目地址</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/05/17/实战项目之爬取ganji网/" data-id="ciob5ugq10005im2zst5evwvb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2016/05/17/python实战之一发送电子邮件/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">python实战之一发送电子邮件</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/05/17/实战项目之爬取ganji网/">实战项目之爬取ganji网</a>
          </li>
        
          <li>
            <a href="/2016/05/17/python实战之一发送电子邮件/">python实战之一发送电子邮件</a>
          </li>
        
          <li>
            <a href="/2016/05/16/python之数据结构/">python之数据结构</a>
          </li>
        
          <li>
            <a href="/2016/05/15/macdown语法简介/">macdown语法简介</a>
          </li>
        
          <li>
            <a href="/2016/05/15/爬虫实践教程3-多进程/">爬虫实践教程3-多进程</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Zhang Tony<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>