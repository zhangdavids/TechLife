<!DOCTYPE html>
<html lang="zh-Hans">


<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, user-scalable=no">
  <title>
    实战项目之爬取ganji网 | 上药三品，神与气精
  </title>
  <meta name="description" content="曾因酒醉鞭名马 生怕情多累美人">
  
  <meta name="keywords" content="
  python
  ">
  
  <meta name="author" content="John Cheung">

  <meta http-equiv="Cache-Control" content="no-transform"/>
  <meta http-equiv="Cache-Control" content="no-siteapp">

  <link rel="icon" type="image/x-icon" href="undefined">
  <link rel="stylesheet" href="/zhangdavids.github.io/css/main.css">
  <link rel="stylesheet"
        href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  

  

  <script src="//cdnjs.cloudflare.com/ajax/libs/vue/1.0.25-csp/vue.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.11.2/moment.min.js"></script>
</head>

<body id="replica-app">

<nav class="navbar-wrapper">
  <div class="navbar">
    <div class="container clearfix">
      <a href="/zhangdavids.github.io/" class="navbar-logo"><i class="fa fa-github"></i></a>

      <div class="navbar-search float-left desktop-only">
        <div class="navbar-search-form">
          <label for="gsc-i-id1">This website</label>
          <div id="google-search">
            <gcse:search></gcse:search>
          </div>
        </div>
      </div>

      <ul class="navbar-nav float-left">
        
        <li><a href="/zhangdavids.github.io/archives">Archives</a></li>
        
        
        <li><a href="/zhangdavids.github.io/categories">Categories</a></li>
        
        
        <li><a href="/zhangdavids.github.io/tags">Tags</a></li>
        
        
        <li class="desktop-only"><a href="/zhangdavids.github.io/atom.xml" target="_blank">RSS</a></li>
        
      </ul>

      <ul class="navbar-nav user-nav float-right desktop-only">
        <li class="user-nav-notification">
          <a><span class="user-nav-unread"></span><i class="fa fa-bell"></i></a>
        </li>
        <li>
          <a><i class="fa fa-plus"></i> <i class="fa fa-caret-down"></i></a>
        </li>
        <li class="user-nav-logo">
          <a><img src="/images/pingan.jpeg"> <i class="fa fa-caret-down"></i></i></a>
        </li>
      </ul>
    </div>
  </div>
</nav>

<div class="main-container">
  <header class="header-wrapper desktop-only">
  <div class="container header-site-detail">
    <ul class="header-toolbar">
      <li class="clearfix">
        <a href="/zhangdavids.github.io/archives" class="header-toolbar-left"><i
                  class="fa fa-file-text"></i> Posts </a>
        <a href="/zhangdavids.github.io/archives"
           class="header-toolbar-right"> 320 </a>
      </li>
      <li>
        <a href="/zhangdavids.github.io/tags" class="header-toolbar-left"><i
                  class="fa fa-tags"></i> Tags </a>
        <a href="/zhangdavids.github.io/tags"
           class="header-toolbar-right"> 29 </a>
      </li>
      <li>
        <a href="/zhangdavids.github.io/categories" class="header-toolbar-left"><i
                  class="fa fa-folder-open"></i> Categories </a>
        <a href="/zhangdavids.github.io/categories"
           class="header-toolbar-right"> 24 </a>
      </li>
    </ul>
    <h2 class="header-title">
      <i class="fa fa-book text-muted"></i>
      <a href="/zhangdavids.github.io/">上药三品，神与气精</a>
      
      
    </h2>
  </div>

  <div class="container">
    <div class="header-tab-wrapper clearfix">
      <span class="header-tab header-tab-selected"><i class="fa fa-thumbs-o-up"></i> Like</span>
      <span class="header-tab"><i class="fa fa-share-alt"></i> Share</span>
      <span class="header-tab"><i class="fa fa-comments-o"></i> Discussion</span>
      <span class="header-tab"><i class="fa fa-bookmark-o"></i> Bookmark </span>
      <span class="header-tab"><i class="fa fa-smile-o"></i> Smile <i class="fa fa-caret-down"></i></span>
    </div>
  </div>
</header>


<div class="post-container container">
  <h3>
    <i class="fa fa-user-o"></i>
    John Cheung

    <span class="post-date float-right" title="{{moment(1463454364000).format('MMM DD, YYYY, h:mm:ss A')}}">
      <i class="fa fa-pencil-square-o"></i>
      {{moment(1463454364000).fromNow()}}
    </span>
  </h3>

  <article class="post-content">
    <h1>实战项目之爬取ganji网</h1>
    <p>爬取分类信息网站龙头  ganji网。  </p>
<p>首先呢，来看看结构<br><img src="http://o7b8j5zne.bkt.clouddn.com/ganji%E5%9B%BE1.png" alt="图1"> </p>
<p>我们需要来看看首页那里的所有类目，里面包含20项<br>基本上所有的帖子都会被分类在这20个下方，根据时间被分配不同的id，像这样<br><a href="http://bj.ganji.com/ershoubijibendiannao/2111373477x.htm" target="_blank" rel="external">http://bj.ganji.com/ershoubijibendiannao/2111373477x.htm</a><br>当然后面加了个x。。。  </p>
<ul>
<li>第一步<br>先把这20个类目的地址爬取出来。</li>
</ul>
<pre><code class="python"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup
<span class="keyword">import</span> requests


start_url = <span class="string">'http://bj.ganji.com/wu/'</span>
url_host = <span class="string">'http://bj.ganji.com'</span>

<span class="function"><span class="keyword">def</span> <span class="title">get_index_url</span><span class="params">(url)</span>:</span>
    <span class="comment"># url = start_url</span>
    wb_data = requests.get(url)
    soup = BeautifulSoup(wb_data.text, <span class="string">'lxml'</span>)
    links = soup.select(<span class="string">'.fenlei &gt; dt &gt; a'</span>)
    <span class="keyword">for</span> link <span class="keyword">in</span> links:
        page_url = url_host + link.get(<span class="string">'href'</span>)
        print(page_url)

get_index_url(start_url)
</code></pre>
<p>程序的运行结果是20个链接，  </p>
<pre><code class="python">http://bj.ganji.com/jiaju/
http://bj.ganji.com/rirongbaihuo/
http://bj.ganji.com/shouji/
http://bj.ganji.com/shoujihaoma/
http://bj.ganji.com/bangong/
http://bj.ganji.com/nongyongpin/
http://bj.ganji.com/jiadian/
http://bj.ganji.com/ershoubijibendiannao/
http://bj.ganji.com/ruanjiantushu/
http://bj.ganji.com/yingyouyunfu/
http://bj.ganji.com/diannao/
http://bj.ganji.com/xianzhilipin/
http://bj.ganji.com/fushixiaobaxuemao/
http://bj.ganji.com/meironghuazhuang/
http://bj.ganji.com/shuma/
http://bj.ganji.com/laonianyongpin/
http://bj.ganji.com/xuniwupin/
http://bj.ganji.com/qitawupin/
http://bj.ganji.com/ershoufree/
http://bj.ganji.com/wupinjiaohuan/
</code></pre>
<p>把这20个链接放入程序中，成为channel_list。<br>这也就是我们的第一个小程序 channel_extracing.py。  </p>
<hr>
<ul>
<li>第二步<br>从大的分类下获取单个帖子的链接，先来分析下页面结构，<br><img src="http://o7b8j5zne.bkt.clouddn.com/ganji%E5%9B%BE2.png" alt="图2"> </li>
</ul>
<p>以二手笔记本电脑为例，可以看到后面结构为o＋page，这里o代表的是个人发布的，后面是page。<br>那么首先考虑的是构造一个爬虫来抓取links</p>
<pre><code class="python"><span class="comment"># spider 1</span>
<span class="function"><span class="keyword">def</span> <span class="title">get_links_from</span><span class="params">(channel, pages, who_sells=<span class="string">'o'</span>)</span>:</span>
    <span class="comment"># http://bj.ganji.com/ershoubijibendiannao/o3/</span>
    <span class="comment"># o for personal a for merchant</span>
    list_view = <span class="string">'{}{}{}/'</span>.format(channel, str(who_sells), str(pages))
    wb_data = requests.get(list_view,headers=headers,proxies=proxies)
    soup = BeautifulSoup(wb_data.text, <span class="string">'lxml'</span>)
    <span class="keyword">if</span> soup.find(<span class="string">'ul'</span>, <span class="string">'pageLink'</span>):
        <span class="keyword">for</span> link <span class="keyword">in</span> soup.select(<span class="string">'dd.feature div ul li a'</span>):
            item_link = link.get(<span class="string">'href'</span>)
            <span class="keyword">if</span> item_link != <span class="string">"javascript:"</span>:                
                url_list.insert_one({<span class="string">'url'</span>: item_link})
                print(item_link)
            <span class="comment"># return urls</span>
    <span class="keyword">else</span>:
        <span class="comment"># It's the last page !</span>
        <span class="keyword">pass</span>
</code></pre>
<p>这里注意一下加了一个判断，判断这一页不是最后一页，就是看下方有无pageLink，俗称也就是翻页器类似的，还有就是直接加上了headers部分和proxy部分反爬取。  </p>
<p>函数的结果是urls，并考虑使用mongo数据库来存储这个url_list。  </p>
<p>接下来就是访问单个的url来抓取里面的数据信息了。  </p>
<pre><code class="python"><span class="comment"># spider 2</span>
<span class="function"><span class="keyword">def</span> <span class="title">get_item_info_from</span><span class="params">(url,data=None)</span>:</span>
    wb_data = requests.get(url,headers=headers)
    <span class="keyword">if</span> wb_data.status_code == <span class="number">404</span>:
        <span class="keyword">pass</span>
    <span class="keyword">else</span>:
        soup = BeautifulSoup(wb_data.text, <span class="string">'lxml'</span>)
        data = {
            <span class="string">'title'</span>:soup.title.text.strip(),
            <span class="string">'price'</span>:soup.select(<span class="string">'.f22.fc-orange.f-type'</span>)[<span class="number">0</span>].text.strip(),
            <span class="string">'pub_date'</span>:soup.select(<span class="string">'.pr-5'</span>)[<span class="number">0</span>].text.strip().split(<span class="string">' '</span>)[<span class="number">0</span>],
            <span class="string">'area'</span>:list(map(<span class="keyword">lambda</span> x:x.text,soup.select(<span class="string">'ul.det-infor &gt; li:nth-of-type(3) &gt; a'</span>))),
            <span class="string">'cates'</span>:list(soup.select(<span class="string">'ul.det-infor &gt; li:nth-of-type(1) &gt; span'</span>)[<span class="number">0</span>].stripped_strings),
            <span class="string">'url'</span>:url
        }
        print(data)
        item_info.insert_one(data)
</code></pre>
<p>看图，单个页面我们考虑抓取，标题title 价格price 发布时间pub_data 交易地点area 类型cates 另外再考虑把url也加入里面。大概先这样咯。<br><img src="http://o7b8j5zne.bkt.clouddn.com/ganji%E5%9B%BE3.png" alt="图3">  </p>
<p>把程序组织一下 组成第二个py程序 page_parsing.py<br>当然详情页抓取的数据我们也放到mongo的数据库中。  </p>
<p>到这里初步小结一下，实现了哪些呢？先是从ganji的首页获取分类，20个分类<br>然后在这20个分类下，获取单个帖子的详情页，然后通过详情页去抓取想要的数据。<br>爬取的初步流程完成了，接下来就是包装，<br>写个主函数，定义下我们抓取多少页的大分类，先url_list然后item_info都存入到mongo的数据库中。<br>ps 考虑写一个计数的函数观察我们抓取的信息数目。  </p>
<hr>
<ul>
<li>第三步<br>主函数的分析   </li>
</ul>
<pre><code class="python"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool  
<span class="keyword">from</span> page_parsing <span class="keyword">import</span> get_item_info_from,url_list,item_info,get_links_from  
<span class="keyword">from</span> channel_extracing <span class="keyword">import</span> channel_list
</code></pre>
<p>也就是前面的两个函数，我们实现了几个功能：</p>
<p>channel_list<br>两个方法 get_item_info_from     get_links_from<br>两个mongo数据库的collections url_list item_info  </p>
<p>另外这里使用多进程来加快抓取的速度      </p>
<pre><code class="python"><span class="function"><span class="keyword">def</span> <span class="title">get_all_links_from</span><span class="params">(channel)</span>:</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">100</span>):
        get_links_from(channel,i)
</code></pre>
<p>注意前面定义的函数我们定义了页面，这里抓取1-99页，当然也许部分分类商品多，99不足，部分分类商品不是很多，我们也已经有判断会pass掉。。  </p>
<pre><code class="python"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:
    pool = Pool(processes=<span class="number">6</span>)
    <span class="comment"># pool = Pool()</span>
    pool.map(get_all_links_from,channel_list.split())
    pool.close()
    pool.join()
</code></pre>
<p>开6个进程进行爬爬爬，先存url，再根据url访问抓取。    </p>
<hr>
<ul>
<li>第四步<br>写上一个计数的函数，这样可以在运行main函数存入数据库时并打印链接的同时，统计已经写了多少数据进去。  </li>
</ul>
<pre><code class="python"><span class="keyword">import</span> time
<span class="keyword">from</span> page_parsing <span class="keyword">import</span> url_list

<span class="keyword">while</span> <span class="keyword">True</span>:
    print(url_list.find().count())
    time.sleep(<span class="number">5</span>)
</code></pre>
<hr>
<ul>
<li>最后<br>附上项目github地址    <a href="https://github.com/zhangdavids/myganji" target="_blank" rel="external">项目地址</a></li>
</ul>

  </article>
</div>


    




</div>

<div class="footer-wrapper container">
  <footer class="footer clearfix">
    <div class="clearfix">
    <a href="https://zhangdavids.github.io" class="footer-logo">
      <i class="fa fa-github"></i>
    </a>
    <ul class="footer-social-link">
      <li>© 2018 John Cheung</li>
      <li><a href="https://zhangdavids.github.io">Home</a></li>
      
    </ul>
    <div class="footer-theme-info">
      Theme <a href="//github.com/sabrinaluo/hexo-theme-replica">Replica</a>
      by <a href="//github.com/sabrinaluo">Hiitea</a> ❤ Powered by Hexo
    </div>
    </div>
    
  </footer>
</div>




<script src="/zhangdavids.github.io/js/main.js"></script>

</body>
</html>
