<!DOCTYPE html>
<html lang="zh-Hans">


<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, user-scalable=no">
  <title>
    kaggle总结 | 上药三品，神与气精
  </title>
  <meta name="description" content="曾因酒醉鞭名马 生怕情多累美人">
  
  <meta name="keywords" content="
  kaggle
  ">
  
  <meta name="author" content="John Cheung">

  <meta http-equiv="Cache-Control" content="no-transform"/>
  <meta http-equiv="Cache-Control" content="no-siteapp">

  <link rel="icon" type="image/x-icon" href="undefined">
  <link rel="stylesheet" href="/tech/css/main.css">
  <link rel="stylesheet"
        href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  

  

  <script src="//cdnjs.cloudflare.com/ajax/libs/vue/1.0.25-csp/vue.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.11.2/moment.min.js"></script>
</head>

<body id="replica-app">

<nav class="navbar-wrapper">
  <div class="navbar">
    <div class="container clearfix">
      <a href="/tech/" class="navbar-logo"><i class="fa fa-github"></i></a>

      <div class="navbar-search float-left desktop-only">
        <div class="navbar-search-form">
          <label for="gsc-i-id1">This website</label>
          <div id="google-search">
            <gcse:search></gcse:search>
          </div>
        </div>
      </div>

      <ul class="navbar-nav float-left">
        
        <li><a href="/tech/archives">Archives</a></li>
        
        
        <li><a href="/tech/categories">Categories</a></li>
        
        
        <li><a href="/tech/tags">Tags</a></li>
        
        
        <li class="desktop-only"><a href="/tech/atom.xml" target="_blank">RSS</a></li>
        
      </ul>

      <ul class="navbar-nav user-nav float-right desktop-only">
        <li class="user-nav-notification">
          <a><span class="user-nav-unread"></span><i class="fa fa-bell"></i></a>
        </li>
        <li>
          <a><i class="fa fa-plus"></i> <i class="fa fa-caret-down"></i></a>
        </li>
        <li class="user-nav-logo">
          <a><img src="/images/pingan.jpeg"> <i class="fa fa-caret-down"></i></i></a>
        </li>
      </ul>
    </div>
  </div>
</nav>

<div class="main-container">
  <header class="header-wrapper desktop-only">
  <div class="container header-site-detail">
    <ul class="header-toolbar">
      <li class="clearfix">
        <a href="/tech/archives" class="header-toolbar-left"><i
                  class="fa fa-file-text"></i> Posts </a>
        <a href="/tech/archives"
           class="header-toolbar-right"> 318 </a>
      </li>
      <li>
        <a href="/tech/tags" class="header-toolbar-left"><i
                  class="fa fa-tags"></i> Tags </a>
        <a href="/tech/tags"
           class="header-toolbar-right"> 28 </a>
      </li>
      <li>
        <a href="/tech/categories" class="header-toolbar-left"><i
                  class="fa fa-folder-open"></i> Categories </a>
        <a href="/tech/categories"
           class="header-toolbar-right"> 24 </a>
      </li>
    </ul>
    <h2 class="header-title">
      <i class="fa fa-book text-muted"></i>
      <a href="/tech/">上药三品，神与气精</a>
      
      
    </h2>
  </div>

  <div class="container">
    <div class="header-tab-wrapper clearfix">
      <span class="header-tab header-tab-selected"><i class="fa fa-thumbs-o-up"></i> Like</span>
      <span class="header-tab"><i class="fa fa-share-alt"></i> Share</span>
      <span class="header-tab"><i class="fa fa-comments-o"></i> Discussion</span>
      <span class="header-tab"><i class="fa fa-bookmark-o"></i> Bookmark </span>
      <span class="header-tab"><i class="fa fa-smile-o"></i> Smile <i class="fa fa-caret-down"></i></span>
    </div>
  </div>
</header>


<div class="post-container container">
  <h3>
    <i class="fa fa-user-o"></i>
    John Cheung

    <span class="post-date float-right" title="{{moment(1548807921000).format('MMM DD, YYYY, h:mm:ss A')}}">
      <i class="fa fa-pencil-square-o"></i>
      {{moment(1548807921000).fromNow()}}
    </span>
  </h3>

  <article class="post-content">
    <h1>kaggle总结</h1>
    <p>主要是机器学习的过程</p>
<p>采用scikit-learn 结合pandas numpy matplotlib seaborn</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">input_df = pd.read_csv(&apos;data/raw/train.csv&apos;, header=0)</span><br><span class="line">submit_df  = pd.read_csv(&apos;data/raw/test.csv&apos;,  header=0)</span><br><span class="line"></span><br><span class="line"># 合并他们</span><br><span class="line">df = pd.concat([input_df, submit_df])</span><br><span class="line"></span><br><span class="line"># 重建index</span><br><span class="line">df.reset_index(inplace=True)</span><br><span class="line"></span><br><span class="line"># 删除reset_index()产生的index column</span><br><span class="line">df.drop(&apos;index&apos;, axis=1, inplace=True)</span><br><span class="line"></span><br><span class="line">print df.shape[1], &quot;columns:&quot;, df.columns.values</span><br><span class="line">print &quot;Row count:&quot;, df.shape[0]</span><br></pre></td></tr></table></figure>
<p>输出如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">12 columns: [&apos;PassengerId&apos; &apos;Survived&apos; &apos;Pclass&apos; &apos;Name&apos; &apos;Sex&apos; &apos;Age&apos; &apos;SibSp&apos; &apos;Parch&apos;</span><br><span class="line"> &apos;Ticket&apos; &apos;Fare&apos; &apos;Cabin&apos; &apos;Embarked&apos;]</span><br><span class="line">Row count: 1309</span><br></pre></td></tr></table></figure>
<p>可以看到有12个特征</p>
<p>查看数据的完整性情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def observe(df):</span><br><span class="line">	print &quot;column: &quot;, df.shape[1]</span><br><span class="line">	columns = df.columns</span><br><span class="line">	for i in columns:</span><br><span class="line">		print i, &quot;missing &quot;,pd.isnull(df[i]).sum(), &quot; type:&quot;, df[i].dtypes</span><br></pre></td></tr></table></figure>
<ul>
<li>Cabin 缺失很严重，我想可以忽略这一个特征了。</li>
<li>Age 缺失的并不多，而且Age是一个重要的特征，应该保留。</li>
</ul>
<h3 id="如何处理缺失的数据"><a href="#如何处理缺失的数据" class="headerlink" title="如何处理缺失的数据"></a>如何处理缺失的数据</h3><ol>
<li>直接扔掉出现缺失Value的数据：只有少量的数据出现缺失Value的情况，这样做比较简单快捷。</li>
<li>给缺失的Value赋特殊值来表明它是缺失的：比较适用于分类变量，因为缺失Value就是不存在的数据，如果给他分配平均值之类的数值并没有什么意义。除非是某些潜在原因使某些缺失值会影响其与另外一个值的关联(correlation)。并且这种方法不适用于连续变量。不过对于二元变量(binary variables)，我们可以把他的缺失值赋为0，正常情况下True为1，False为-1。</li>
<li>给缺失的Value赋平均值：这种简单的做法很普遍，对于不重要的特征来说用这种方法足矣。还可以结合其他变量来算平均值。对于分类变量，使用最常见的值或许比平均值更好。</li>
<li>使用机器学习算法/模型来预测缺失数据：感觉只有数据量很大的情况下这样做才有效。</li>
</ol>
<h3 id="定量转换"><a href="#定量转换" class="headerlink" title="定量转换"></a>定量转换</h3><p>变量转换的目的是将数据转换为模型适用的格式，不同方法实现的随机森林(Random Forest)接受不同类型的数据，Scikit-learn要求数据都是数字型numeric，所以我们要将原始数据转换为数字型numeric。</p>
<p>所有的数据可以分为两类：1.定性(Quantitative)变量可以以某种方式排序，Age就是一个很好的列子。2.定量(Qualitative)变量描述了物体的某一（不能被数学表示的）方面，Embarked就是一个例子。</p>
<p>Dummy Variables<br>就是类别变量或者二元变量，当qualitative variable是一些频繁出现的几个独立变量时，Dummy Variables比较适合使用。我们以Embarked为例，Embarked只包含三个值’S’,’C’,’Q’，我们可以使用下面的代码将其转换为dummies:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">embark_dummies  = pd.get_dummies(df[&apos;Embarked&apos;])</span><br><span class="line">df = df.join(embark_dummies)</span><br><span class="line">df.drop([&apos;Embarked&apos;], axis=1,inplace=True)</span><br></pre></td></tr></table></figure>
<p>Factorizing<br>dummy不好处理Cabin（船舱号）这种标称属性，因为他出现的变量比较多。所以Pandas有一个方法叫做factorize()，它可以创建一些数字，来表示类别变量，对每一个类别映射一个ID，这种映射最后只生成一个特征，不像dummy那样生成多个特征。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line"># Replace missing values with &quot;U0&quot;</span><br><span class="line">df[&apos;Cabin&apos;][df.Cabin.isnull()] = &apos;U0&apos;</span><br><span class="line"></span><br><span class="line"># create feature for the alphabetical part of the cabin number</span><br><span class="line">df[&apos;CabinLetter&apos;] = df[&apos;Cabin&apos;].map( lambda x : re.compile(&quot;([a-zA-Z]+)&quot;).search(x).group())</span><br><span class="line"></span><br><span class="line"># convert the distinct cabin letters with incremental integer values</span><br><span class="line">df[&apos;CabinLetter&apos;] = pd.factorize(df[&apos;CabinLetter&apos;])[0]</span><br></pre></td></tr></table></figure>
<p>Scaling<br>Scaling可以将一个很大范围的数值映射到一个很小的范围(通常是-1 - 1，或则是0 - 1)，很多情况下我们需要将数值做Scaling使其范围大小一样，否则大范围数值特征将会由更高的权重。比如：Age的范围可能只是0-100，而income的范围可能是0-10000000，在某些对数组大小敏感的模型中会影响其结果。</p>
<p>下面的代码是对Age进行Scaling：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># StandardScaler will subtract the mean from each value then scale to the unit variance</span><br><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">df[&apos;Age_scaled&apos;] = scaler.fit_transform(df[&apos;Age&apos;])</span><br></pre></td></tr></table></figure>
<p>Binning<br>Binning通过观察“邻居”(即周围的值)来连续数据离散化。存储的值被分布到一些“桶”或箱中，就像直方图的bin将数据划分成几块一样。下面的代码对Fare进行Binning。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Divide all fares into quartiles</span><br><span class="line">df[&apos;Fare_bin&apos;] = pd.qcut(df[&apos;Fare&apos;], 4)</span><br><span class="line"></span><br><span class="line"># qcut() creates a new variable that identifies the quartile range, but we can&apos;t use the string so either</span><br><span class="line"># factorize or create dummies from the result</span><br><span class="line">df[&apos;Fare_bin_id&apos;] = pd.factorize(df[&apos;Fare_bin&apos;])</span><br><span class="line">df = pd.concat([df, pd.get_dummies(df[&apos;Fare_bin&apos;]).rename(columns=lambda x: &apos;Fare_&apos; + str(x))], axis=1)</span><br></pre></td></tr></table></figure>
<p>特征提取很重要的一个方面是深入理解数据，并且能提取出新的特征来做预测。机器学习的核心就是模型选取和参数选择，特征提取可以说是重中之重。</p>
<p>一个特征提取的例子是，从电话号码中提取中国家、地区、城市的信息，或者是从GPS中提取中国家、地区、城市的信息。只要能描述一个事物的qualitative变量，都有可能从中挖掘出有用的特征，另外，时序等信息也是非常有用的。</p>
<p>泰坦尼克号的这些数据非常简单，我们并不需要对数据做太多的处理，我们下面只对name，cabin和ticket提取一些变量。</p>
<ul>
<li>name 提取称呼</li>
<li>Cabin<br>客舱信息包含了甲板和房间号，不同甲板位置不同，逃生船数量不同，人群年龄分布不同等等。不同房间号离甲板距离不同，离逃生船距离不同，等等。所以从客舱中提取中甲板和房间号这两个特征很重要。</li>
</ul>
<p>机器学习的模型很多，用于分类有：</p>
<ol>
<li>回归算法：Logistic Regression、 Ordinary Least Square等等。</li>
<li>决策树: CART、ID3、Random Forest等等。</li>
<li>贝叶斯：Navie Bayesian、BBN等等。</li>
<li>基于实例的算法：KNN、LVQ等等。</li>
<li>组合模型、关联规则、神经网络、深度学习等等。<br>模型太多都看晕了，这种场景下选什么模型合适？</li>
</ol>
<p>随机森林</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">X = df[:input_df.shape[0]].values[:, 1::]</span><br><span class="line">y = df[:input_df.shape[0]].values[:, 0]</span><br><span class="line"></span><br><span class="line">X_test = df[input_df.shape[0]:].values[:, 1::]</span><br><span class="line">random_forest = RandomForestClassifier(oob_score=True, n_estimators=1000)</span><br><span class="line">random_forest.fit(X, y)</span><br><span class="line"></span><br><span class="line">Y_pred = random_forest.predict(X_test)</span><br><span class="line">print random_forest.score(X, y)</span><br><span class="line">submission = pd.DataFrame(&#123;</span><br><span class="line">	    &quot;PassengerId&quot;: X_origin[&quot;PassengerId&quot;],</span><br><span class="line">	    &quot;Survived&quot;: Y_pred.astype(int)</span><br><span class="line">	&#125;)</span><br><span class="line">submission.to_csv(&apos;result.csv&apos;, index=False)</span><br></pre></td></tr></table></figure>
<p>GBDT</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import GradientBoostingClassifier</span><br><span class="line">X = df[:input_df.shape[0]].values[:, 1::]</span><br><span class="line">y = df[:input_df.shape[0]].values[:, 0]</span><br><span class="line"></span><br><span class="line">X_test = df[input_df.shape[0]:].values[:, 1::]</span><br><span class="line">GBDT = GradientBoostingClassifier(n_estimators=1000)</span><br><span class="line">GBDT.fit(X, y)</span><br><span class="line"></span><br><span class="line">Y_pred = GBDT.predict(X_test)</span><br><span class="line">print GBDT.score(X, y)</span><br><span class="line">submission = pd.DataFrame(&#123;</span><br><span class="line">	    &quot;PassengerId&quot;: X_origin[&quot;PassengerId&quot;],</span><br><span class="line">	    &quot;Survived&quot;: Y_pred.astype(int)</span><br><span class="line">	&#125;)</span><br><span class="line">submission.to_csv(&apos;result2.csv&apos;, index=False)</span><br></pre></td></tr></table></figure>
<h3 id="调优-优化"><a href="#调优-优化" class="headerlink" title="调优 优化"></a>调优 优化</h3><p>再观察一下数据，看看还有那些特征可以用到，又去Google了一番，整理出三个新特征：称谓、家庭大小、姓。</p>
<p>称谓：不同的称谓意味着不同的社会地位、不同的社会地位的人对人生、事物的理解不同。并且不同的社会地位乘坐逃生舱的概率也不同？可能某一类人的生存概率更高？</p>
<p>家庭大小：一家七个人的逃生概率大还是一家两个人的逃生概率大呢？人多的家庭会不会更难逃生呢？</p>
<p>姓：其实姓这个特征是为了辅助家庭这个特征的，同一个姓是一个家庭的概率更大？</p>
<p>参数调优，Sklean提供了两种方法，GridSearch和RandomizedSearch。在这两种情况下，都可以指定每个参数的取值范围，创建一个字典。将参数字典提供给search方法，它就会执行模型所指定的值的组合。GridSearch会测试参数每一个可能的组合。 而RandomizedSearch需要指定有多少不同的组合要测试，然后随机选择并组合他们。</p>
<p>使用Random Forest, 加上参数max_depth=5 防止模型过拟合，并将n_estimators放到了30000</p>
<p>首先 Error = Bias + Variance，Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。<br>举一个例子，一次打靶实验，目标是为了打到10环，但是实际上只打到了7环，那么这里面的Error就是3。具体分析打到7环的原因，可能有两方面：一是瞄准出了问题，比如实际上射击瞄准的是9环而不是10环；二是枪本身的稳定性有问题，虽然瞄准的是9环，但是只打到了7环。那么在上面一次射击实验中，Bias就是1,反应的是模型期望与真实目标的差距，而在这次试验中，由于Variance所带来的误差就是2，即虽然瞄准的是9环，但由于本身模型缺乏稳定性，造成了实际结果与模型期望之间的差距。</p>
<p>High variance，low bias意味着”overfitting”，模型过拟合导致不能很好的用于新数据。而High bias，low variance意味着”underfitting”，模型欠拟合导致不能很好从样本中学习，很难去预测新数据。Bias与Variance往往是不能兼得的。如果要降低模型的Bias，就一定程度上会提高模型的Variance，反之亦然。</p>
<p>例如，如果模型存在high variance，一个常见的解决方法是给他增加更多的特征。但是这样也会增加bias，这中间的平衡需要仔细考虑。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.learning_curve import learning_curve</span><br><span class="line">def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,</span><br><span class="line">                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Generate a simple plot of the test and traning learning curve.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    estimator : object type that implements the &quot;fit&quot; and &quot;predict&quot; methods</span><br><span class="line">        An object of that type which is cloned for each validation.</span><br><span class="line"></span><br><span class="line">    title : string</span><br><span class="line">        Title for the chart.</span><br><span class="line"></span><br><span class="line">    X : array-like, shape (n_samples, n_features)</span><br><span class="line">        Training vector, where n_samples is the number of samples and</span><br><span class="line">        n_features is the number of features.</span><br><span class="line"></span><br><span class="line">    y : array-like, shape (n_samples) or (n_samples, n_features), optional</span><br><span class="line">        Target relative to X for classification or regression;</span><br><span class="line">        None for unsupervised learning.</span><br><span class="line"></span><br><span class="line">    ylim : tuple, shape (ymin, ymax), optional</span><br><span class="line">        Defines minimum and maximum yvalues plotted.</span><br><span class="line"></span><br><span class="line">    cv : integer, cross-validation generator, optional</span><br><span class="line">        If an integer is passed, it is the number of folds (defaults to 3).</span><br><span class="line">        Specific cross-validation objects can be passed, see</span><br><span class="line">        sklearn.cross_validation module for the list of possible objects</span><br><span class="line"></span><br><span class="line">    n_jobs : integer, optional</span><br><span class="line">        Number of jobs to run in parallel (default 1).</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(title)</span><br><span class="line">    if ylim is not None:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(&quot;Training examples&quot;)</span><br><span class="line">    plt.ylabel(&quot;Score&quot;)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=1)</span><br><span class="line">    train_scores_std = np.std(train_scores, axis=1)</span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=1)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=1)</span><br><span class="line">    plt.grid()</span><br><span class="line"></span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=0.1,</span><br><span class="line">                     color=&quot;r&quot;)</span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std, alpha=0.1, color=&quot;g&quot;)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, &apos;o-&apos;, color=&quot;r&quot;,</span><br><span class="line">             label=&quot;Training score&quot;)</span><br><span class="line">    plt.plot(train_sizes, test_scores_mean, &apos;o-&apos;, color=&quot;g&quot;,</span><br><span class="line">             label=&quot;Cross-validation score&quot;)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=&quot;best&quot;)</span><br><span class="line">    return plt</span><br><span class="line">title = &quot;Learning Curves&quot;</span><br><span class="line">plot_learning_curve(RandomForestClassifier(oob_score=True, n_estimators=30000, max_depth=5), title, X, y, ylim=(0.5, 1.01), cv=None, n_jobs=4, train_sizes=[50, 100, 150, 200, 250, 350, 400])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

  </article>
</div>


    




</div>

<div class="footer-wrapper container">
  <footer class="footer clearfix">
    <div class="clearfix">
    <a href="https://zhangdavids.github.io/tech" class="footer-logo">
      <i class="fa fa-github"></i>
    </a>
    <ul class="footer-social-link">
      <li>© 2018 John Cheung</li>
      <li><a href="https://zhangdavids.github.io/tech">Home</a></li>
      
    </ul>
    <div class="footer-theme-info">
      Theme <a href="//github.com/sabrinaluo/hexo-theme-replica">Replica</a>
      by <a href="//github.com/sabrinaluo">Hiitea</a> ❤ Powered by Hexo
    </div>
    </div>
    
  </footer>
</div>




<script src="/tech/js/main.js"></script>

</body>
</html>
